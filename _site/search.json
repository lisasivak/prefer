[
  {
    "objectID": "funding.html",
    "href": "funding.html",
    "title": "Funding",
    "section": "",
    "text": "This work is generously supported by a VIDI grant (VI.Vidi.201.119) from the Netherlands Organization for Scientific Research (NWO) to Gert Stulp. The LISS panel data were collected by Centerdata (Tilburg University, The Netherlands) through its MESS project funded by the Netherlands Organization for Scientific Research. The ODISSEI Benchmark Platform, the ODISSEI-SICSS Summer School, and the development of the LISS harmonized dataset are financed by the ODISSEI Roadmap Project financed by NWO."
  },
  {
    "objectID": "details/overview/5special_issue.html",
    "href": "details/overview/5special_issue.html",
    "title": "Special issue and community paper",
    "section": "",
    "text": "We plan to publish a community paper presenting the design and results of the PreFer data challenge. Everyone who was part of a team that made a working submission at least in one phase of the challenge will be invited to be a co-author of this paper. By a working submission we mean a submitted method that produced predictions for the holdout set and that is accompanied by a description of the method. There will be no limit on the number of participants who can qualify as co-authors.\nAdditionally, we plan to publish a special issue on the results of the data challenge in the Journal of computational social science. All the participants of the data challenge will be invited to submit a manuscript to this special issue. The submitted papers will be peer-reviewed.\nThe call for papers with detailed instructions and requirements will be published later on this website. A paper should describe the process that led to the final submission. This includes for example decisions concerning data preprocessing and handling missing data, model and variable selection, and what was learned during this process. A paper can also be aimed at describing how the data challenge contributed to fertility research. Other ideas will also be possible after discussing them with the challenge organisers. Manuscripts need to be accompanied by a clearly documented modular open-source code that will allow other researchers to reproduce all the results, as well as figures and tables in the article."
  },
  {
    "objectID": "details/overview/3application.html",
    "href": "details/overview/3application.html",
    "title": "Application",
    "section": "",
    "text": "To apply for the data challenge, please fill in this google form.\nYou can participate individually or as part of a team. If you participate as a team, all team members should fill in the application form.\nWho can apply to participate in PreFer?\nAnyone can apply, regardless of the field of work or study.\nPhase 1 and track 3 of Phase 2 of the data challenge are open for everyone. There will be a winner determined based on the results of Phase 1/track 3 of Phase 2. All of the participants who make at least one working submission in any phase and track of the data challenge will get an opportunity to co-author a community paper about the challenge or submit a paper to the special issue.\nCertain requirements must be met to participate in tracks 1-2 of Phase 2 of PreFer. During this phase of the challenge, participants of tracks 1-2 will have access to the Dutch registry data. The access to this data is governed by strict rules and regulations in relation to data protection and privacy. Only a limited number of Phase 1 participants will get the opportunity to participate in these tracks. See details on how we will select the teams here."
  },
  {
    "objectID": "details/overview/2data.html",
    "href": "details/overview/2data.html",
    "title": "Data",
    "section": "",
    "text": "This challenge uses two data sources: survey data from the LISS panel and linked register data from Statistics Netherlands (Bakker et al., 2014), or CBS.\n\nLISS datasets\nThe LISS panel is based on a traditional probability sample drawn from the Dutch population register by Statistics Netherlands and is managed by the non-profit research institute Centerdata. The LISS panel started in 2007 when approximately 5000 households comprising 8000 individuals of 16 years and older were recruited.\nMembers of the panel participate in ten longitudinal surveys (or Core Study modules) on ten different topics (family, health, religion, social integration and leisure, work and education, personality, politics and values, assets, income, housing). Most of these surveys are conducted annually, and some biannually. Each time, each survey measures the same set of variables.\nAnother source of data on the LISS panel is the Background survey. It is filled out by a household’s contact person when the household joins the panel and is updated monthly. It collects basic socio-demographic information about the household and all of its members (including those who are not LISS panel members and do not participate in the Core surveys).\nWe made several datasets based on these sources of data. See details about all the datasets based on the LISS panel data [here]/posts/posts/2024-03-20-prefer-datasets.qmd){target=“_blank”}.\nThe main training dataset includes people aged 18-45 years old in 2020, who participated at least in one Core study survey in 2007-2020. Overall, there are ~6900 people in this group. However, most of them have dropped out of the LISS panel by 2021-2023, and because of that the outcome - getting a new child in 2021-2023 - is available only for ~1400 of them.\nWe splitted all individuals for whom the outcome is available into a training set and holdout (test) set. Because some respondents come from the same households, we first splitted the households into train and holdout to avoid data leakage. See more details on the train-test split in the paper).\nAll the datasets will be provided to the participants of the data challenge after sighing the user statement.\nRead further:\n- More detailed description of all LISS datasets provided for participants of the challenge\n- Some examples of how to make use of these different LISS datasets - Tips about how to use codebooks for these LISS datasets\n- More guides that help work with LISS data.\n\n\n\nCBS dataset\nThe linked register data comes from population registries of the Netherlands collected by Statistics Netherlands (CBS) (we will refer to this source as CBS data). Participants will have access to a selection of all CBS datasets. These datasets include information about marriages and partnerships, children, education, employment, income and assets, neighborhood characteristics and more, as well as social networks for the whole Dutch population van der Laan et al., 2023. Most of these datasets cover the period from 1995 to 2023, but only the data up to 2020 can be used for training.\nBased on these selected datasets, a base preprocessed dataset (mostly with the data from 2020) will be prepared and made available to the participants as a starter package along with a codebook in Dutch and English. This preprocessed file includes only our target group – Dutch residents aged 18-45 in 2020 (more than 6 million people). Participants will be able to calculate additional variables based on the full longitudinal datasets and also add characteristics of their networks, using the network datasets for the linkage. The scripts with examples of how to preprocess the network datasets and calculate variables about the structure and composition of one’s networks will be provided. To allow adding the characteristics of the network and because in this phase participants will submit predictions (not only the code), only the outcome variable for the test set is held out (see the figure below).\nAdditional CBS datasets (not initially available to the participants of the challenge) can be requested throughout the challenge with a short justification of why the dataset is requested. The relevant CBS datasets can be searched using the CBS micro-data catalogue and ODISSEI portal. Data from external sources (not included in the CBS datasets) that can be linked to groups of individuals can also be uploaded if approved by CBS – for example, welfare policies by municipality.\nNB: because the access to the CBS data is governed by strict rules and regulations in relation to data protection and privacy, only a limited number of participants (10-20 teams, one person per team), selected to participate in tracks 1-2 of Phase 2, will get access to the CBS data provided that they pass security checks and get approved by CBS. For those participants, the costs of access to the CBS datasets will be covered by ODISSEI. You can read about how teams will be selected to participate in Phase 2 and the conditions for accessing the CBS data here."
  },
  {
    "objectID": "posts/posts/2024-03-25-simple-walkthrough-r.html",
    "href": "posts/posts/2024-03-25-simple-walkthrough-r.html",
    "title": "A walkthough of submitting a simple model in R",
    "section": "",
    "text": "Here we describe how to prepare and make a submission in R. The sole purpose of this script is to make the submission process more clear, not to present a model that is any good.\nIn this example, we assume that you did all the prerequisite steps described here. You have forked and cloned (e.g. downloaded) the GitHub repository, and now have a folder with all the files from this repository on your computer. We will call this folder your “local repository”.\nLet’s imagine that you want to add one predictor to the basic model that is already in the repository: respondents’ gender (variable name gender_bg, as you found using the codebooks). To produce this model you should use the template functions that are already in the repository: clean_df for preprocessing the data from the “submission.R” script, train_save_model from the “training.R” script, and predict_outcomes from the “submission.R” script (to test your model and preprocessing).\nOverall steps: reading in the data —&gt; preprocessing the data —&gt; training and saving the model —&gt; testing on the fake data —&gt; editing/saving “submission.R”, “training.R”, “packages.R” accordingly —&gt; adding a short description of the method to “description.md” —&gt; pushing your materials to the online Github repository -&gt; submitting."
  },
  {
    "objectID": "posts/posts/2024-03-25-simple-walkthrough-r.html#reading-in-data",
    "href": "posts/posts/2024-03-25-simple-walkthrough-r.html#reading-in-data",
    "title": "A walkthough of submitting a simple model in R",
    "section": "Reading-in data",
    "text": "Reading-in data\n\nRead-in the training data and outcome. IMPORTANT: it is strongly advised to save the PreFer datafiles in a different folder than your local repository. The reason is that these datasets cannot be made public, and when you save the datasets in your local repository you may accidentally upload the datasets to your online repository when you “push” your latest changes. This would constitute a serious data breach.\n\nThe code to read-in your data is the only code that you do not need to document through your repository.\n\n# loading data (predictors)\nlibrary(data.table) # requires install.packages(\"data.table\") first\ntrain &lt;- data.table::fread(\"path to the data which is NOT in your local repository/PreFer_train_data.csv\",\n                           keepLeadingZeros = TRUE, # if FALSE adds zeroes to some dates\n                           data.table = FALSE)\n# base R's read.csv is also possible but is ssssllloooowww\n\n# loading the outcome\noutcome &lt;- data.table::fread(\"path to the data which is NOT in your local repository/PreFer_train_outcome.csv\",\n                             data.table = FALSE)"
  },
  {
    "objectID": "posts/posts/2024-03-25-simple-walkthrough-r.html#preprocessing-and-training",
    "href": "posts/posts/2024-03-25-simple-walkthrough-r.html#preprocessing-and-training",
    "title": "A walkthough of submitting a simple model in R",
    "section": "Preprocessing and training",
    "text": "Preprocessing and training\n\nFind your folder with the PreFer materials, and open the submission.R script. Edit the clean_df function: add the new variable:\n\n\nclean_df &lt;- function(df, background_df = NULL) {\n    # Process the input data to feed the model\n  \n    # calculate age   \n    df$age &lt;- 2024 - df$birthyear_bg\n\n    # Selecting variables for modelling\n\n    keepcols = c('nomem_encr', # ID variable required for predictions,\n                 'age', \n                 'gender_bg')  # &lt;-------- ADDED VARIABLE 'gender_bg'\n  \n    # Keeping data with variables selected\n    df &lt;- df[ , keepcols ]\n    \n    # turning gender into factor\n    df$gender_bg&lt;- as.factor(df$gender_bg) # &lt;-------- ADDED THIS\n\n    return(df)\n}\n\nNow your clean_df function is done. Make sure to save it and load it into your R environment.\n\nEdit the train_save_model function from the “training.R”: add the new variable:\n\n\ntrain_save_model &lt;- function(cleaned_df, outcome_df) {\n  # Trains a model using the cleaned dataframe and saves the model to a file.\n\n  # Parameters:\n  # cleaned_df (dataframe): The cleaned data from clean_df function to be used for training the model.\n  # outcome_df (dataframe): The data with the outcome variable (e.g., from PreFer_train_outcome.csv or PreFer_fake_outcome.csv).\n\n  ## This script contains a bare minimum working example\n  set.seed(1) # not useful here because logistic regression deterministic\n  \n  # Combine cleaned_df and outcome_df\n  model_df &lt;- merge(cleaned_df, outcome_df, by = \"nomem_encr\")\n  \n  # Logistic regression model\n  model &lt;- glm(new_child ~ age + gender_bg, family = \"binomial\", data = model_df) # &lt;-------- ADDED VARIABLE 'gender_bg'\n  \n  # Save the model\n  saveRDS(model, \"model.rds\")\n}\n\nNow your train_save_model function is done. Make sure to save it and load it into your R environment.\n\nPreprocess the data using your updated clean_df function, and then train the model via train_save_model. If you are working in a R Markdown or R Notebook document in RStudio, or when you have opened an Rproject (recommended!), the model (model.rds) will be saved in the same folder as your scripts – in your local repository. If you are working via an R script, you will probably need to manually set the local repository as the working directory.\n\n\n# setwd(\"path to your local repository\") # &lt;---- provide the path here\n\n# preprocessing the data\ntrain_cleaned &lt;- clean_df(train)\n\n# training and saving the model\ntrain_save_model(train_cleaned, outcome)\n\nYour model is trained, and saved in model.rds."
  },
  {
    "objectID": "posts/posts/2024-03-25-simple-walkthrough-r.html#testing-on-fake-data",
    "href": "posts/posts/2024-03-25-simple-walkthrough-r.html#testing-on-fake-data",
    "title": "A walkthough of submitting a simple model in R",
    "section": "Testing on fake data",
    "text": "Testing on fake data\n\nTest the preprocessing function and model on fake data to see if they will run on the holdout set. If your method does not run on the “fake data”, it will not run on the holdout data. [If you “push” your method to Github this test will also be automatically run].\n\nTo do this test you can edit the function predict_outcomes from “submission.R”. Load the fake data (it is already in your local repository) and apply the predict_outcomes.\n\n# load the fake data\nfake &lt;- data.table::fread(\"PreFer_fake_data.csv\",\n                          keepLeadingZeros = TRUE, # if FALSE adds zeroes to some dates\n                          data.table = FALSE)\n\npredict_outcomes &lt;- function(df, background_df = NULL, model_path = \"./model.rds\"){\n  # Generate predictions using the saved model and the input dataframe.\n    \n  # The predict_outcomes function accepts a dataframe as an argument\n  # and returns a new dataframe with two columns: nomem_encr and\n  # prediction. The nomem_encr column in the new dataframe replicates the\n  # corresponding column from the input dataframe The prediction\n  # column contains predictions for each corresponding nomem_encr. Each\n  # prediction is represented as a binary value: '0' indicates that the\n  # individual did not have a child during 2021-2023, while '1' implies that\n  # they did.\n  \n  # Parameters:\n  # df (dataframe): The data dataframe for which predictions are to be made.\n  # background_df (dataframe): The background data dataframe for which predictions are to be made.\n  # model_path (str): The path to the saved model file (which is the output of training.R).\n\n  # Returns:\n  # dataframe: A dataframe containing the identifiers and their corresponding predictions.\n  \n  ## This script contains a bare minimum working example\n  if( !(\"nomem_encr\" %in% colnames(df)) ) {\n    warning(\"The identifier variable 'nomem_encr' should be in the dataset\")\n  }\n\n  # Load the model\n  model &lt;- readRDS(model_path)\n    \n  # Preprocess the fake / holdout data\n  df &lt;- clean_df(df, background_df)\n\n  # Exclude the variable nomem_encr if this variable is NOT in your model\n  vars_without_id &lt;- colnames(df)[colnames(df) != \"nomem_encr\"]\n  \n  # Generate predictions from model\n  predictions &lt;- predict(model, \n                         subset(df, select = vars_without_id), \n                         type = \"response\") \n  \n  # Create predictions that should be 0s and 1s rather than, e.g., probabilities\n  predictions &lt;- ifelse(predictions &gt; 0.5, 1, 0)  \n  \n  # Output file should be data.frame with two columns, nomem_encr and predictions\n  df_predict &lt;- data.frame(\"nomem_encr\" = df[ , \"nomem_encr\" ], \"prediction\" = predictions)\n  # Force columnnames (overrides names that may be given by `predict`)\n  names(df_predict) &lt;- c(\"nomem_encr\", \"prediction\") \n  \n  # Return only dataset with predictions and identifier\n  return( df_predict )\n}\n\n\n# apply the function to the fake data\npredict_outcomes(fake)\n\nIf you get a data.frame including predictions, your test on the fake data has passed!"
  },
  {
    "objectID": "posts/posts/2024-03-25-simple-walkthrough-r.html#editsave-files-for-submission",
    "href": "posts/posts/2024-03-25-simple-walkthrough-r.html#editsave-files-for-submission",
    "title": "A walkthough of submitting a simple model in R",
    "section": "Edit/save files for submission",
    "text": "Edit/save files for submission\nYou can now prepare the files for submission, that will be applied to the holdout set:\n\nEdit/Save the clean_df function in your “submission.R”. This code will be applied to the holdout data. You don’t need to adapt the predict_outcomes function in “submission.R” because the outputs of your model are predicted classes already (i.e., 0s and 1s).\nprediction model: make sure that your model is saved in the same folder as submission.R under the name model.rds.\n“packages.R”: you don’t have to edit this file now, because you didn’t used any packages.\nEdit/Save the train_save_model function in the “training.R” script.\nWhen you think your all set, it is advised to test the entire workflow by running Rscript run.R PreFer_fake_data.csv PreFer_fake_background_data.csv from the command line / terminal."
  },
  {
    "objectID": "posts/posts/2024-03-25-simple-walkthrough-r.html#adding-a-description",
    "href": "posts/posts/2024-03-25-simple-walkthrough-r.html#adding-a-description",
    "title": "A walkthough of submitting a simple model in R",
    "section": "Adding a description",
    "text": "Adding a description\n\nAdd a brief description of your method to the file description.md (e.g. “binary logistic regression with two variables - age and gender - selected manually”)"
  },
  {
    "objectID": "posts/posts/2024-03-25-simple-walkthrough-r.html#update-online-github-repository",
    "href": "posts/posts/2024-03-25-simple-walkthrough-r.html#update-online-github-repository",
    "title": "A walkthough of submitting a simple model in R",
    "section": "Update online GitHub repository",
    "text": "Update online GitHub repository\nNow you need to update your online GitHub repository. You can do it in several ways. Here we assume that you used GitHub Desktop for cloning the repository and will also use it to commit (i.e. capture the state of the local repository at that point in time) and push the changes (e.g. change the online repository):\n\nGo to GitHub Desktop and press “Commit to master”. You need to add some description (e.g. “add gender”).\n\n\n\nPush the changes (“Push origin”) (i.e. update your online repository) - press “Push origin” on the upper right.\nNow go to the “Actions” tab in you online github repository. After a few minutes you’ll see if your submission passed the automatic checks."
  },
  {
    "objectID": "posts/posts/2024-03-25-simple-walkthrough-r.html#submit-your-method",
    "href": "posts/posts/2024-03-25-simple-walkthrough-r.html#submit-your-method",
    "title": "A walkthough of submitting a simple model in R",
    "section": "Submit your method",
    "text": "Submit your method\n\nSubmit your method as explained here.\n\nIMPORTANT: always save the code that you used to produce the model via the train_save_model function. Eventhough this function will not be run on the holdout data, we [the PreFer organisers] will use it to ensure reproducibility and verify whether the predictions you submitted are the same as the predictions that arise from your code stored in train_save_model.\nPhoto by Fotis Fotopoulos on Unsplash | Photo by Kelli McClintock on Unsplash"
  },
  {
    "objectID": "posts/posts/2024-03-24-first-empty-submission.html",
    "href": "posts/posts/2024-03-24-first-empty-submission.html",
    "title": "My first (empty) submission",
    "section": "",
    "text": "Here we describe how you, before you change any code, can make your first submission in the PreFer data challenge. This is useful because you can see how the online (Github) tests work.\nWe will only change the description.md file. Let’s just write “my first test” in the file. We can do that in multiple ways:\n\nVia Github.com\nLet’s first do it via a browser.\n\nGo to your forked repository, something like https://github.com/YOURNAME/fertility-prediction-challenge\nSelect the description.md file.\nClick on the pencil button in the top right corner\nAdd your text to the description file\nClick on “Commit changes” in the green button in the top right corner\nCommit your changes with a useful message.\n\nAfter the commit, your submission (which will be a simple one variable logistic regression as described in submission.py / submission.R) will be evaluated on the fake data for testing. In your Github Actions [and only after you have previously clicked “I understand my workflows, go ahead and enable them.”] you should now see that a test has been started, and it will give you a green checkmark after a couple of minutes.\n\n\nMaking changes on your computer and push to github\nHere we assume that you have “cloned” your repository, and that all relevant files live in some folder on your computer.\n\nGo to the folder where you have stored the PreFer materials.\nOpen the description.md file and add any text, and save your file.\n“Commit” the changes through something like Github Desktop, RStudio, or the command line.\n“Push” your changes to your online repository. IMPORTANT: make sure that you don’t push any of the PreFer datasets to your repository! These dataset cannot be shared. It is a safe strategy to keep your datasets in a different folder on your computer.\n\nAfter the push, your submission (which will be a simple one variable logistic regression as described in submission.py / submission.R) will be evaluated on the fake data for testing. In your Github Actions [and only after you have previously clicked “I understand my workflows, go ahead and enable them.”] you should now see that a test has been started, and it will give you a green checkmark after a couple of minutes.\nPhoto by MJH SHIKDER on Unsplash | Photo by Kelli McClintock on Unsplash"
  },
  {
    "objectID": "posts/posts/2024-03-21-prefer-codebooks.html",
    "href": "posts/posts/2024-03-21-prefer-codebooks.html",
    "title": "A guide on PreFer codebooks (LISS)",
    "section": "",
    "text": "Here we describe how to make use the codebooks PreFer_codebook.csv and PreFer_codebook_summary.csv. Broadly speaking, the first codebook is most useful to examine the meaning and characteristics of individual variables and their values and the second is most useful to see how the measurement of particular variables may have changed over time.\nThere are also detailed codebooks in Dutch and English on the LISS website. They contain important information, e.g. about filtering, which can be useful for working with missing values.\nHere we describe how to use the PreFer codebooks."
  },
  {
    "objectID": "posts/posts/2024-03-21-prefer-codebooks.html#finding-variables",
    "href": "posts/posts/2024-03-21-prefer-codebooks.html#finding-variables",
    "title": "A guide on PreFer codebooks (LISS)",
    "section": "Finding variables",
    "text": "Finding variables\nAn important aim of the codebook is to get insight on individual variables.\n\nExample 1\nImagine you are interested in what the variable cp12e007 means. You can just open the codebook in an editor and look for it:\n\nYou can also do that via R or Python. In R:\n\nlibrary(readr) # for nicer output tables in html\ncodebook &lt;- readr::read_csv(\"../../data/PreFer_codebook.csv\") \n# you can also find this codebook at preferdatachallenge.nl/data/PreFer_codebook.csv\n\ncodebook[ codebook$var_name == \"cp12e007\", ]\n\n# A tibble: 1 × 12\n  var_name var_label             values_cat labels_cat unique_values_n n_missing\n  &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 cp12e007 I receive far too ma… 1; 2; 3; … 1 totally…               7         1\n# ℹ 6 more variables: prop_missing &lt;dbl&gt;, type_var &lt;chr&gt;, note &lt;chr&gt;,\n#   year &lt;dbl&gt;, survey &lt;chr&gt;, dataset &lt;chr&gt;\n\n\nThis variable contained an answer to the question “I receive far too many requests to participate in surveys.”. It came from the 2012 Personality core survey. This is a categorical variable and has 7 possible values (from 1 to 7, see values_cat) which reflects a Likert scale because it has value labels (1 totally disagree; 2; 3; 4; 5; 6; 7 totally agree; see labels_cat). In the data, 7 unique values were actually observed (see unique_values_n), which does not need to be the case. In this 2012 wave of this Personality core survey, there was only 1 missing value (n_missing), which meant 0.0001679825 was missing (and that 1/0.0001679825 = 5953 participated in the survey). There is no note meaning no problems were observed for this particular variable.\n\n\nExample 2\nLet’s examine whether any variable labels (or question labels) say anything about income.\n\ncodebook[ grepl(\"income\", codebook$var_label), ] # 1192 variables!\n\n# A tibble: 1,192 × 12\n   var_name var_label            values_cat labels_cat unique_values_n n_missing\n   &lt;chr&gt;    &lt;chr&gt;                &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n 1 ci08a008 Did you receive inc… 1; 2       yes; no                  2        50\n 2 ci09b008 Did you receive inc… 1; 2       yes; no                  2        27\n 3 ci10c008 Did you receive inc… 1; 2       yes; no                  2        62\n 4 ci11d008 Did you receive inc… 1; 2       yes; no                  2        87\n 5 ci12e008 Did you receive inc… 1; 2       yes; no                  2        40\n 6 ci13f008 Did you receive inc… 1; 2       yes; no                  2        81\n 7 ci14g008 Did you receive inc… 1; 2       yes; no                  2        41\n 8 ci15h008 Did you receive inc… 1; 2       yes; no                  2        68\n 9 ci16i008 Did you receive inc… 1; 2       yes; no                  2        32\n10 ci17j008 Did you receive inc… 1; 2       yes; no                  2        32\n# ℹ 1,182 more rows\n# ℹ 6 more variables: prop_missing &lt;dbl&gt;, type_var &lt;chr&gt;, note &lt;chr&gt;,\n#   year &lt;dbl&gt;, survey &lt;chr&gt;, dataset &lt;chr&gt;\n\n\nLet’s focus on the variable netinc:\n\ncodebook[ codebook$var_name == \"netinc\", ]\n\n# A tibble: 1 × 12\n  var_name var_label             values_cat labels_cat unique_values_n n_missing\n  &lt;chr&gt;    &lt;chr&gt;                 &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 netinc   Personal net monthly… &lt;NA&gt;       &lt;NA&gt;                  3810    426190\n# ℹ 6 more variables: prop_missing &lt;dbl&gt;, type_var &lt;chr&gt;, note &lt;chr&gt;,\n#   year &lt;dbl&gt;, survey &lt;chr&gt;, dataset &lt;chr&gt;\n\n\nThis variable measures personal net monthly income in euro. This is a numeric variable, and about 24% is missing. This is a background variable which means that it is recorded monthly for respondents. This variable can be found in PreFer_train_background_data.csv. At one point in time this variable had value labels associated with it: Unknown (missing) = -15, Prefer not to say = -14, I dont know = -13. However, these values were never observed in the actual data, which is why we removed these value labels from the variable var_label and why we turned this variable into numeric (from categorical). This process is described in note."
  },
  {
    "objectID": "posts/posts/2024-03-21-prefer-codebooks.html#variable-types",
    "href": "posts/posts/2024-03-21-prefer-codebooks.html#variable-types",
    "title": "A guide on PreFer codebooks (LISS)",
    "section": "Variable types",
    "text": "Variable types\nMost variables types are either categorical (meaning that values have labels associated with them, e.g., 1 = “Male”, 2 = “Female”) or numeric.\n\ntable(codebook$type_var)\n\n\n                                 categorical \n                                       23571 \ncharacter [almost exclusively empty strings] \n                                          15 \n                                date or time \n                                         566 \n                                     numeric \n                                        6436 \n             response to open-ended question \n                                        1079 \n\n\nLet’s imagine that in our analyses we only want to select variables that are categorical, numeric, or date or time. Note that we want to select these variable in the Core survey data (the train data) and do not want to include variables from the Background data (for further explanation on the datasets, click here).\n\nselect_type &lt;- c(\"categorical\", \"numeric\", \"date or time\")\nlist_variables &lt;- codebook[ (codebook$type_var %in% select_type) & \n                              codebook$dataset == \"PreFer_train_data.csv\", ]$var_name\n\nWe can now use this list of variables in our data. Let’s use the PreFer_fake_train.csv which is a fake dataset that has randomly generated information for 30 cases with identical variable names and structures of variables.\n\nfake_data &lt;- readr::read_csv(\"../../data/PreFer_fake_data.csv\")\n# can also be found preferdatachallenge.nl/data/PreFer_fake_data.csv\nfake_data_sel &lt;- fake_data[ , list_variables ]\nhead(fake_data_sel) # brief overview\n\n# A tibble: 6 × 30,540\n  nomem_encr outcome_available cf08a_m cf09b_m cf10c_m cf11d_m cf12e_m cf13f_m\n       &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     700001                 1  200803  200904  172453  201104  201203  201303\n2     700002                 1  200806  200904   78071  201104  201203  201303\n3     700003                 1  200806  200904  151277  201103  201204  201303\n4     700004                 1  200806  200903  139537  201104  201204  201304\n5     700005                 1  200806  200903   99522  201104  201203  201304\n6     700006                 1  200806  200904  186163  201103  201203  201304\n# ℹ 30,532 more variables: cf14g_m &lt;dbl&gt;, cf15h_m &lt;dbl&gt;, cf16i_m &lt;dbl&gt;,\n#   cf17j_m &lt;dbl&gt;, cf18k_m &lt;dbl&gt;, cf19l_m &lt;dbl&gt;, cf20m_m &lt;dbl&gt;, cf08a001 &lt;dbl&gt;,\n#   cf09b001 &lt;dbl&gt;, cf10c001 &lt;dbl&gt;, cf11d001 &lt;dbl&gt;, cf12e001 &lt;dbl&gt;,\n#   cf13f001 &lt;dbl&gt;, cf14g001 &lt;dbl&gt;, cf15h001 &lt;dbl&gt;, cf16i001 &lt;dbl&gt;,\n#   cf17j001 &lt;dbl&gt;, cf18k001 &lt;dbl&gt;, cf19l001 &lt;dbl&gt;, cf20m001 &lt;dbl&gt;,\n#   cf08a002 &lt;dbl&gt;, cf09b002 &lt;dbl&gt;, cf10c002 &lt;dbl&gt;, cf11d002 &lt;dbl&gt;,\n#   cf12e002 &lt;dbl&gt;, cf13f002 &lt;dbl&gt;, cf14g002 &lt;dbl&gt;, cf15h002 &lt;dbl&gt;, …"
  },
  {
    "objectID": "posts/posts/2024-03-21-prefer-codebooks.html#using-variable-information",
    "href": "posts/posts/2024-03-21-prefer-codebooks.html#using-variable-information",
    "title": "A guide on PreFer codebooks (LISS)",
    "section": "Using variable information",
    "text": "Using variable information\nIn an earlier example, we observed that variable cp12e007 was a categorical variable, but from the labels_cat it was clear that this was some sort of Likert scale, from “1 totally disagree; 2; 3; 4; 5; 6; 7 totally agree” (separated by semi-colons). Let’s read-in and parse the values from values_cat and the corresponding labels from labels_cat\n\nvalues_cp12e007 &lt;- codebook[ codebook$var_name == \"cp12e007\", ]$values_cat # is a string\nvalues_cp12e007 &lt;- strsplit(values_cp12e007, \"; \")[[1]] # to vector\nlabels_cp12e007 &lt;- codebook[ codebook$var_name == \"cp12e007\", ]$labels_cat # is a string\nlabels_cp12e007 &lt;- strsplit(labels_cp12e007, \"; \")[[1]] # to vector\n\n# add information to variable in data\nfake_data_sel$cp12e007 &lt;- factor(fake_data_sel$cp12e007, \n                                 levels = values_cp12e007, labels = labels_cp12e007,\n                                 ordered = TRUE)\nattributes(fake_data_sel$cp12e007)\n\n$levels\n[1] \"1 totally disagree\" \"2\"                  \"3\"                 \n[4] \"4\"                  \"5\"                  \"6\"                 \n[7] \" 7 totally agree\"  \n\n$class\n[1] \"ordered\" \"factor\""
  },
  {
    "objectID": "posts/posts/2024-03-21-prefer-codebooks.html#example-interpretation",
    "href": "posts/posts/2024-03-21-prefer-codebooks.html#example-interpretation",
    "title": "A guide on PreFer codebooks (LISS)",
    "section": "Example interpretation",
    "text": "Example interpretation\nHere we examine the variable with the extension 007 both in the “Family & Household” and “Personality” core surveys.\n\ncodebook_summ &lt;- readr::read_csv(\"../../data/PreFer_codebook_summary.csv\")\n# can also be found at preferdatachallenge.nl/data/PreFer_codebook_summary.csv\nsubset(codebook_summ, \n       last3letters == \"007\" & survey %in% c(\"Family & Household\", \"Personality\") )\n\n# A tibble: 2 × 11\n  survey        last3letters var_names n_waves which_waves n_different_var_label\n  &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;\n1 Family & Hou… 007          cf08a007…      13 2008, 2009…                     2\n2 Personality   007          cp08a007…      10 2008, 2009…                     1\n# ℹ 5 more variables: var_label_across_waves &lt;chr&gt;,\n#   n_different_value_labels &lt;dbl&gt;, value_labels_across_waves &lt;chr&gt;,\n#   n_different_var_type &lt;dbl&gt;, var_type_across_waves &lt;chr&gt;\n\n\nThe variable with the extension (last 3 letters/digits) 007 in the Family & Household survey (“cf”) featured in 13 waves, from 2008 to 2020. It had the following names across those 13 waves: cf08a007, cf09b007, cf10c007, cf11d007, cf12e007, cf13f007, cf14g007, cf15h007, cf16i007, cf17j007, cf18k007, cf19l007, cf20m007. The label of this variable was changed once because there are two labels for these variables: 1) Is your father still alive? and 2) Is your biological father still alive? The answer categories were: yes = 1, no = 2, I don’t know = 99 across all waves.\nThe variable with the same extension (last 3 letters/digits) 007 in the Personality survey (“cp”) featured in 10 waves, from 2008 to 2018. It had the following names across those 10 waves: cp08a007, cp09b007, cp10c007, cp11d007, cp12e007, cp13f007, cp14g007, cp15h007, cp17i007, cp18j007. The label of this variable across waves was “I receive far too many requests to participate in surveys”. Across all waves, there has been a change in labelling: 1 totally disagree = 1, 2 = 2, 3 = 3, 4 = 4, 5 = 5, 6 = 6, 7 totally agree = 7 – AND – 1 totally disagree = 1, 2 = 2, 3 = 3, 4 = 4, 5 = 5, 6 = 6, 7 totally agree = 7. Note the extra space before the ” 7 totally agree” in the first instance."
  },
  {
    "objectID": "posts/posts/2024-03-26-simple-walkthrough-python.html",
    "href": "posts/posts/2024-03-26-simple-walkthrough-python.html",
    "title": "A walkthough of submitting a simple model in python",
    "section": "",
    "text": "Here we describe how to prepare and make a submission in Python. The sole purpose of this script is to make the submission process more clear, not to present a model that is any good.\nIn this example, we assume that you did all the prerequisite steps described here. You have forked and cloned (e.g. downloaded) the GitHub repository, and now have a folder with all the files from this repository on your computer. We will call this folder your “local repository”.\nLet’s imagine that you want to add one predictor to the basic model that is already in the repository: respondents’ gender (variable name gender_bg, as you found using the codebooks). To produce this model you should use the template functions that are already in the repository: clean_df for preprocessing the data from the “submission.py” script, train_save_model from the “training.py” script, and predict_outcomes from the “submission.py” script (to test your model and preprocessing).\nOverall steps: reading in the data —&gt; preprocessing the data —&gt; training and saving the model —&gt; testing on the fake data —&gt; editing/saving “submission.py”, “training.py”, “packages.py” accordingly —&gt; adding a short description of the method to “description.md” —&gt; pushing your materials to the online Github repository -&gt; submitting."
  },
  {
    "objectID": "posts/posts/2024-03-26-simple-walkthrough-python.html#reading-in-data",
    "href": "posts/posts/2024-03-26-simple-walkthrough-python.html#reading-in-data",
    "title": "A walkthough of submitting a simple model in python",
    "section": "Reading-in data",
    "text": "Reading-in data\n\nRead-in the training data and outcome. IMPORTANT: it is strongly advised to save the PreFer datafiles in a different folder than your local repository. The reason is that these datasets cannot be made public, and when you save the datasets in your local repository you may accidentally upload the datasets to your online repository when you “push” your latest changes. This would constitute a serious data breach.\n\nThe code to read-in your data is the only code that you do not need to document through your repository.\n```{python}\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nimport joblib\n\n# loading data (predictors)\ntrain = pd.read_csv(\"path to the data which is NOT in your local repository\\\\PreFer_train_data.csv\", low_memory = False) \n# loading the outcome\noutcome = pd.read_csv(\"path to the data which is NOT in your local repository\\\\PreFer_train_outcome.csv\") \n```"
  },
  {
    "objectID": "posts/posts/2024-03-26-simple-walkthrough-python.html#preprocessing-and-training",
    "href": "posts/posts/2024-03-26-simple-walkthrough-python.html#preprocessing-and-training",
    "title": "A walkthough of submitting a simple model in python",
    "section": "Preprocessing and training",
    "text": "Preprocessing and training\n\nFind your folder with the PreFer materials, and open the submission.py script. Edit the clean_df function: add the new variable:\n\n```{python}\ndef clean_df(df, background_df=None):\n    \"\"\"\n    Preprocess the input dataframe to feed the model.\n    # If no cleaning is done (e.g. if all the cleaning is done in a pipeline) leave only the \"return df\" command\n\n    Parameters:\n    df (pd.DataFrame): The input dataframe containing the raw data (e.g., from PreFer_train_data.csv or PreFer_fake_data.csv).\n    background (pd.DataFrame): Optional input dataframe containing background data (e.g., from PreFer_train_background_data.csv or PreFer_fake_background_data.csv).\n\n    Returns:\n    pd.DataFrame: The cleaned dataframe with only the necessary columns and processed variables.\n    \"\"\"\n\n    ## This script contains a bare minimum working example\n    # Create new variable with age\n    df[\"age\"] = 2024 - df[\"birthyear_bg\"]\n\n    # Imputing missing values in age with the mean\n    df[\"age\"] = df[\"age\"].fillna(df[\"age\"].mean())\n\n    # Selecting variables for modelling\n    keepcols = [\n        \"nomem_encr\",  # ID variable required for predictions,\n        \"age\"         # newly created variable\n        ,\"gender_bg\"  # &lt;--------ADDED VARIABLE\n    ] \n\n    # Keeping data with variables selected\n    df = df[keepcols]\n\n    return df\n```\nNow your clean_df function is done.\n\nEdit the train_save_model function from the “training.py”: add the new variable:\n\n```{python}\ndef train_save_model(cleaned_df, outcome_df):\n    \"\"\"\n    Trains a model using the cleaned dataframe and saves the model to a file.\n\n    Parameters:\n    cleaned_df (pd.DataFrame): The cleaned data from clean_df function to be used for training the model.\n    outcome_df (pd.DataFrame): The data with the outcome variable (e.g., from PreFer_train_outcome.csv or PreFer_fake_outcome.csv).\n    \"\"\"\n    \n    ## This script contains a bare minimum working example\n    #random.seed(1) # not useful here because logistic regression deterministic\n    \n    # Combine cleaned_df and outcome_df\n    model_df = pd.merge(cleaned_df, outcome_df, on=\"nomem_encr\")\n\n    # Filter cases for whom the outcome is not available\n    model_df = model_df[~model_df['new_child'].isna()]  \n    \n    # Logistic regression model\n    model = LogisticRegression()\n\n    # Fit the model\n    model.fit(model_df[['age', 'gender_bg']], model_df['new_child']) # &lt;------- ADDED VARIABLE\n\n    # Save the model\n    joblib.dump(model, \"model.joblib\")\n\n```\nNow your train_save_model function is done.\n\nPreprocess the data using your updated clean_df function, and then train the model via train_save_model. If you are working in a R Markdown or R Notebook document in RStudio, or when you have opened an Rproject (recommended!), the model (model.rds) will be saved in the same folder as your scripts – in your local repository. If you are working via an R script, you will probably need to manually set the local repository as the working directory.\nPreprocess the data using your updated clean_df function, and then train the model via train_save_model. If you are using Jupyter Notebook, the model (model.joblib) is now saved in the same folder as the script - in your local repository. If you are using an environment where the folder where the script is located is not set as the working directory by default, you should manually set the local repository as the working directory.\n\n```{python}\n# import os\n# print os.getcwd() &lt;--- this prints the current working directory\n# os.chdir(path to your local repository) #&lt;---- provide the path here\n\n\n# preprocessing the data\ntrain_cleaned = clean_df(train)\n\n# training and saving the model\ntrain_save_model(train_cleaned, outcome)\n```\nYour model is trained, and saved in model.joblib."
  },
  {
    "objectID": "posts/posts/2024-03-26-simple-walkthrough-python.html#testing-on-fake-data",
    "href": "posts/posts/2024-03-26-simple-walkthrough-python.html#testing-on-fake-data",
    "title": "A walkthough of submitting a simple model in python",
    "section": "Testing on fake data",
    "text": "Testing on fake data\n\nTest the preprocessing function and model on fake data to see if they will run on the holdout set. If your method does not run on the “fake data”, it will not run on the holdout data. [If you “push” your method to Github this test will also be automatically run].\n\nTo do this test you can edit the function predict_outcomes from t”submission.py”. Load the fake data (it is already in your local repository) and apply the predict_outcomes.\n```{python}\n# load the data\nfake = pd.read_csv(\"PreFer_fake_data.csv\") \n\ndef predict_outcomes(df, background_df=None, model_path=\"model.joblib\"):\n    \"\"\"Generate predictions using the saved model and the input dataframe.\n\n    The predict_outcomes function accepts a Pandas DataFrame as an argument\n    and returns a new DataFrame with two columns: nomem_encr and\n    prediction. The nomem_encr column in the new DataFrame replicates the\n    corresponding column from the input DataFrame. The prediction\n    column contains predictions for each corresponding nomem_encr. Each\n    prediction is represented as a binary value: '0' indicates that the\n    individual did not have a child during 2021-2023, while '1' implies that\n    they did.\n\n    Parameters:\n    df (pd.DataFrame): The input dataframe for which predictions are to be made.\n    background_df (pd.DataFrame): The background dataframe for which predictions are to be made.\n    model_path (str): The path to the saved model file (which is the output of training.py).\n\n    Returns:\n    pd.DataFrame: A dataframe containing the identifiers and their corresponding predictions.\n    \"\"\"\n\n    ## This script contains a bare minimum working example\n    if \"nomem_encr\" not in df.columns:\n        print(\"The identifier variable 'nomem_encr' should be in the dataset\")\n\n    # Load the model\n    model = joblib.load(model_path)\n\n    # Preprocess the fake / holdout data\n    df = clean_df(df, background_df)\n\n    # Exclude the variable nomem_encr if this variable is NOT in your model\n    vars_without_id = df.columns[df.columns != 'nomem_encr']\n\n    # Generate predictions from model, should be 0 (no child) or 1 (had child)\n    predictions = model.predict(df[vars_without_id])\n\n    # Output file should be DataFrame with two columns, nomem_encr and predictions\n    df_predict = pd.DataFrame(\n        {\"nomem_encr\": df[\"nomem_encr\"], \"prediction\": predictions}\n    )\n\n    # Return only dataset with predictions and identifier\n    return df_predict\n\n# apply the function to the fake data\npredict_outcomes(fake)\n```\nIf you get a data.frame including predictions, your test on the fake data has passed!"
  },
  {
    "objectID": "posts/posts/2024-03-26-simple-walkthrough-python.html#editsave-files-for-submission",
    "href": "posts/posts/2024-03-26-simple-walkthrough-python.html#editsave-files-for-submission",
    "title": "A walkthough of submitting a simple model in python",
    "section": "Edit/save files for submission",
    "text": "Edit/save files for submission\nYou can now prepare the files for submission, that will be applied to the holdout set:\n\nEdit/Save the clean_df function in your “submission.py”. This code will be applied to the holdout data. You don’t need to adapt the predict_outcomes function in “submission.py” because the outputs of your model are predicted classes already (i.e., 0s and 1s).\nprediction model: make sure that your model is saved in the same folder as submission.py under the name model.joblib.\n“environment.yml”: you don’t have to edit this file now, because you didn’t used any packages.\nEdit/Save the train_save_model function in the “training.py” script.\nWhen you think your all set, it is advised to test the entire workflow by running python run.py PreFer_fake_data.csv PreFer_fake_background_data.csv from the command line / terminal."
  },
  {
    "objectID": "posts/posts/2024-03-26-simple-walkthrough-python.html#adding-a-description",
    "href": "posts/posts/2024-03-26-simple-walkthrough-python.html#adding-a-description",
    "title": "A walkthough of submitting a simple model in python",
    "section": "Adding a description",
    "text": "Adding a description\n\nAdd a brief description of your method to the file description.md (e.g. “binary logistic regression with two variables - age and gender - selected manually”)"
  },
  {
    "objectID": "posts/posts/2024-03-26-simple-walkthrough-python.html#update-online-github-repository",
    "href": "posts/posts/2024-03-26-simple-walkthrough-python.html#update-online-github-repository",
    "title": "A walkthough of submitting a simple model in python",
    "section": "Update online GitHub repository",
    "text": "Update online GitHub repository\nNow you need to update your online GitHub repository. You can do it in several ways. Here we assume that you used GitHub Desktop for cloning the repository and will also use it to commit (i.e. capture the state of the local repository at that point in time) and push the changes (e.g. change the online repository):\n\nGo to GitHub Desktop and press “Commit to master”. You need to add some description (e.g. “add gender”).\n\nPush the changes (“Push origin”) (i.e. update your online repository) - press “Push origin” on the upper right.\nNow go to the “Actions” tab in you online github repository. After a few minutes you’ll see if your submission passed the automatic checks."
  },
  {
    "objectID": "posts/posts/2024-03-26-simple-walkthrough-python.html#submit-your-method",
    "href": "posts/posts/2024-03-26-simple-walkthrough-python.html#submit-your-method",
    "title": "A walkthough of submitting a simple model in python",
    "section": "Submit your method",
    "text": "Submit your method\n\nSubmit your method as explained here.\n\nIMPORTANT: always save the code that you used to produce the model via the train_save_model function. Eventhough this function will not be run on the holdout data, we [the PreFer organisers] will use it to ensure reproducibility and verify whether the predictions you submitted are the same as the predictions that arise from your code stored in train_save_model.\nPhoto by Fotis Fotopoulos on Unsplash | Photo by Kelli McClintock on Unsplash"
  },
  {
    "objectID": "work-in-progress.html",
    "href": "work-in-progress.html",
    "title": "Working papers",
    "section": "",
    "text": "Submitted\n\nStulp, G., Verhagen, M.D., Arpino, B. Research note: a plea for reporting out-of-sample predictive ability. Submitted to Demography.\nStulp, G. Describing the Dutch Social Networks and Fertility Study and How to Process it. Accepted at Demographic Research\nVan Tintelen, A. & Stulp, G., Explaining uncertainty in women’s fertility preferences. Submitted to Heliyon\nLangener et al. It’ss all about timing: Exploring different temporal resolutions for analyzing digital phenotyping data. Resubmitted to Advances in Methods and Practices in Psychological Science.\nStulp, G., Xu, X., Top, L., Sivak, L. A data-driven approach towards network effects on fertility. Submitted to Royel Society Open Science\nXu, X. Van den Bosch, A., Gauthier, A., Stulp, G. Measuring the Response Quality of Open-Ended Questions in a Demographic Web Survey using Linguistic Complexity Features. Submitted to GESIS.\nLangener, AM, Bringman, LF, Kas, MJ, Stulp, G. Predicting mood based on the social environment measured through the experience sampling method, digital phenotyping, and social networks. Submitted to Administration and Policy in Mental Health and Mental Health Services Research\nStadel, M, Stulp, G. Langener, AM, Elmer, T, van Duijn, MAJ, Bringmann, LF. Feedback About a Person’s Social Context - Personal Networks and Daily Social Interactions. Submitted to Administration and Policy in Mental Health and Mental Health Services Research\nStulp, G., Van Tintelen, A., Granholm, R. & Lippényi, Z. A microsimulation model of fertility shows that preferences cannot explain why highly educated women remain childless more often. Submit to Population Studies\nTesting the combination of Feeling Safe and Peer Counselling against Formulation-Based Cognitive Behavior Therapy to promote Psychological Wellbeing in People with Persecutory Delusions: Study Protocol for a Randomized Controlled Trial (the Feeling Safe-NL Trial). Authors: Eva Tolmeijer; Felicity Waite; Louise Isham; Laura Bringmann; Robin Timmers; Arjan van den Berg; Hanneke Schuurmans; Anton B. P. Staring; Paul de Bont; Rob van Grunsven; Gert Stulp; Ben Wijnen; Mark van der Gaag; Daniel Freeman; David van den Berg. Submitted to Trials"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Gert Stulp: g.stulp@rug.nl\nElizaveta Sivak: e.sivak@rug.nl"
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html",
    "href": "posts/posts/2024-03-20-prefer-datasets.html",
    "title": "A guide on PreFer LISS datasets",
    "section": "",
    "text": "Here we describe the different datasets that are available in the PreFer data challenge that are all based on the LISS panel. There are five datasets available to PreFer participants: 1) PreFer_train_data.csv, 2) PreFer_train_outcome.csv, 3) PreFer_train_background_data.csv, 4) PreFer_train_supplementary_data.csv, and 5) PreFer_train_supplementary_outcome.csv. The three datasets PreFer_holdout_data.csv, PreFer_holdout_outcome.csv, and PreFer_holdout_background_data.csv will only be available to PreFer organisers. There are also three datasets of fake data, available for participants, that are mostly in place for testing whether your submission will be able to run on the holdout data (PreFer_fake_data.csv, PreFer_fake_outcome.csv, and PreFer_fake_background_data.csv). This all sounds overwhelming, so let’s quickly get to explaining them.\nTo navigate these datasets and get a better understanding of the many variables, there are also machine-readable (and human readable) codebooks available: click here for a guide on how to use the codebooks."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#core-surveys",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#core-surveys",
    "title": "A guide on PreFer LISS datasets",
    "section": "Core surveys",
    "text": "Core surveys\nThe Core study modules are (bi)yearly questionnaires that are targeted at all LISS respondents. This is the list of Core study modules:\n\n“Family & Household/source/”; all waves of Family & Household data, except 2021 and beyond\n“Economic Situation Assets/source/”; all waves of Economic Situation Assets data, except 2021 and beyond\n“Economic Situation Housing/source/”; all waves of Economic Situation Housing data, except 2021 and beyond\n“Economic Situation Income/source/”; all waves of Economic Situation Income data, except 2021 and beyond\n“Health/source/”; all waves of Health data, except 2021 and beyond\n“Personality/source/”; all waves of Personality data, except 2021 and beyond\n“Politics and Values/source/”; all waves of Politics and Values data, except 2021 and beyond\n“Religion and Ethnicity/source/”; all waves of Religion and Ethnicity data, except 2021 and beyond\n“Social Integration and Leisure/source/”; all waves of Social Integration and Leisure data, except 2021 and beyond\n“Work & Schooling/source/”; all waves of Work & Schooling data, except 2021 and beyond\n\nWe have merged all Core surveys from 2007 to 2020.\n\nVariable naming conventions\nAll the variable names in LISS contain information that can help navigate the dataset. Let’s use the variable name cf09b010 as an example. It consists of two main parts: cf09b which indicates the name of the Core study module and the year, and 010 which is a question number in this Core study module (the same questions have the same numbers in all the yearly questionnaires of a Core study module). How to decipher the first part:\n\nc refers to Core study. Almost all the variables in the dataset start with c\nf refers to the name of the Core study module - in this case, it’s “Family and household”. Other options are: w (Work and schooling), s (Social integration and values), h (Health), r (Religion and ethnicity), v (politics and Values), p (Personality), a (economic situation: Assets), i (economic situation: Income), and d (economic situation: Housing).\n09 refers to the year of this questionnaire. 09 means 2009. Some Core study modules are fielded each year, some (Assets) once in two years.\nb refers to the wave number (in this case, Wave 2). Keep in mind, that because some modules are fielded every year, and some every other year, the same letters do not mean that these surveys were conducted in the same year (e.g. Wave 2 of Family and Household module cf09b was conducted in 2009 whereas the second Wave of the Assets core study module ca10b was conducted in 2010).\n\nThe codebooks are particularly useful in navigating the merged Core survey dataset. The datasets PreFer_train_data.csv, PreFer_train_supplement.csv, and PreFer_holdout_data.csv all come from this merged Core survey dataset."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#background",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#background",
    "title": "A guide on PreFer LISS datasets",
    "section": "Background variables",
    "text": "Background variables\nThe Background survey is filled out by a household’s contact person when the household joins the panel and is updated monthly. It collects basic socio-demographic information about the household and all of its members (including those who are not LISS panel members and do not participate in the Core surveys). The following variables are available:\n\nnomem_encr Number of household member encrypted\n\nnohouse_encr Number of household encrypted\n\nwave Year and month of the field work period\n\npositie Position within the household\n\nlftdcat Age in CBS (Statistics Netherlands) categories\n\nlftdhhh Age of the household head\n\naantalhh Number of household members\n\naantalki Number of living-at-home children in the household, children of the household head or his/her partner\n\npartner The household head lives together with a partner (wedded or unwedded)\n\nburgstat Civil status\n\nwoonvorm Domestic situation\n\nwoning Type of dwelling that the household inhabits\n\nbelbezig Primary occupation\n\nbrutoink Personal gross monthly income in Euros\n\nnettoink Personal net monthly income in Euros (incl. nettocat)\n\nbrutocat Personal gross monthly income in categories\n\nnettocat Personal net monthly income in categories\n\noplzon Highest level of education irrespective of diploma\n\noplmet Highest level of education with diploma\n\noplcat Level of education in CBS (Statistics Netherlands) categories\n\ndoetmee Household member participates in the panel\n\nsted Urban character of place of residence\n\nsimpc Does the household have a simPC?\n\nbrutoink_f Personal gross monthly income in Euros, imputed\n\nnetinc Personal net monthly income in Euros\n\nnettoink_f Personal net monthly income in Euros, imputed\n\nbrutohh_f Gross household income in Euros\n\nnettohh_f Net household income in Euros\n\nwerving From which recruitment wave the household originates\n\nbirthyear_imp Year of birth [imputed by PreFer organisers] (based on original gebjaar variable)\n\ngender_imp Gender [imputed by PreFer organisers] (based on original geslacht variable)\n\nmigration_background_imp Origin [imputed by PreFer organisers] (based on original herkomstgroep variable)\n\nage_imp Age of the household member [imputed by PreFer organisers] (based on original leeftijd variable)\n\nThe PreFer datasets PreFer_train_background_data.csv and PreFer_holdout_background_data.csv are based on this background dataset."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_data.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_data.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "1. PreFer_train_data.csv",
    "text": "1. PreFer_train_data.csv\nThe PreFer_train_data.csv consists of 31634 variables from 6418 respondents. These individuals are LISS respondents who were born between 1975 and 2002 (meaning they were between 18 and 45 years old in 2020). These variables are a combination of all Core study modules (see above) and selected yearly background variables.\nParticularly important variables are:\n\nnomem_encr: this is the unique identifier and can be used to link the train data to the other datasets (PreFer_train_background.csv, PreFer_train_supplement.csv, and PreFer_train_outcome.csv). Note that this identifier is a pseudonym of the original nomem_encr identifier in the LISS data.\nnew_child: this is the outcome variable; whether people had a child between 2021 and 2023. Here you can find detailed information on how this outcome was constructed. This dataset also included people for whom this outcome is not known.\n\nLess important, but useful to know:\n\noutcome_available: this is variable that records whether the outcome new_child is available for a particular respondent. 1 means that the outcome is available, 0 means that the outcome is unavailable. Most likely, one would use this variable to restrict the sample to respondents for whom the outcome is available (i.e. outcome_available = 1)."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#selected-yearly-background-variables-added-to-core-surveys",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#selected-yearly-background-variables-added-to-core-surveys",
    "title": "A guide on PreFer LISS datasets",
    "section": "Selected yearly background variables added to Core surveys",
    "text": "Selected yearly background variables added to Core surveys\nThe background variables can be found in a separate dataset (see PreFer_train_background.csv), but we have chosen to include some of the information from the background variables to the Core surveys. Specifically, we have added information from variables that either vary considerably across time or that are useful as fixed variables. Remember that the background variables contain information on a monthly basis; we chose to add 14 yearly variables (for each of the years 2007 up to and including 2020) for a select number of varying background variables.\n\nFixed background variables\nWe included the following fixed variables from the background variables:\n\nbirthyear_bg (birthyear of respondent; which we created and cleaned)\ngender_bg (gender of respondent; which we created and cleaned)\nmigration_background_bg (migration background of the respondent; which we created and cleaned)\nage_bg (the age of the respondent for each wave; which we created; this variable is obviously not fixed, but more or less records the same information as birthyear).\n\n\n\nTime varying background variables\nWith respect to the time varying variables: There are 7 variables that vary over time for households for which yearly variables will be added to the Core dataset:\n\npartner (whether household head lives together with partner)\nwoonvorm (living arrangement)\nburgstat (marital status)\nwoning (type of dwelling)\nsted (urban character of place of residence)\nbrutohh_f (gross household income)\nnettohh_f (net household income).\n\nThere are 9 variables that vary over time for each individual for which yearly variables will be added to the Core dataset\n\nbelbezig (primary occupation)\nbrutoink (personal gross monthly income)\nnettoink (net montly income)\noplzon (highest level of education irrespective of diploma)\noplmet (highest level of education with diploma),\noplcat (level of education in Statistics Netherlands categories)\nbrutoink_f (personal gross monthly income, imputed)\nnetinc (net monthly income)\nnettoink_f (personal net monthly income, imputed).\n\nThus, we include information from 4 fixed variables, and 14 yearly variables (2007 to 2020) for each of the 16 varying variables, totalling 228 variables."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_outcome.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_outcome.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "2. PreFer_train_outcome.csv",
    "text": "2. PreFer_train_outcome.csv\nThis dataset contains the outcome variable for the 6418 respondents in our target groups (born between 1975 and 2002) and it includes only two variables, nomem_encr and the outcome variable new_child (this variables includes missing values). nomem_encr can be used to link this dataset to PreFer_train_data.csv."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_background_data.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_background_data.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "3. PreFer_train_background_data.csv",
    "text": "3. PreFer_train_background_data.csv\nThis dataset consists all 33 variables that are listed in #background and contains information on 12854 respondents from 4950 different households."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_supplementary_data.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_supplementary_data.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "4. PreFer_train_supplementary_data.csv",
    "text": "4. PreFer_train_supplementary_data.csv\nThis dataset contains the same 31634 variables as PreFer_train_data.csv, but it includes data from 10644 respondents outside our target group (i.e., this dataset contains people born prior to 1975 or post 2002)."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_supplementary_outcome.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_train_supplementary_outcome.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "5. PreFer_train_supplementary_outcome.csv",
    "text": "5. PreFer_train_supplementary_outcome.csv\nThis dataset contains the outcome (new_child) for the respondents in PreFer_train_supplementary_data.csv. It can be linked through nomem_encr."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_fake_data.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_fake_data.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "6. PreFer_fake_data.csv",
    "text": "6. PreFer_fake_data.csv\nThis dataset contains the same variables as PreFer_train_data.csv, but it includes fake data with 30 randomly drawn values from each variable in the train and holdout data. This dataset is used for testing whether the developed preprocessing scripts and trained models work. A failed test on this data likely means a failed submission on the holdout data."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_fake_outcome.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_fake_outcome.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "7. PreFer_fake_outcome.csv",
    "text": "7. PreFer_fake_outcome.csv\nA dataset that contains the fake outcomes for the fake data PreFer_fake_data.csv. It can be linked through nomem_encr. This dataset is used for testing whether the developed preprocessing scripts and trained models work. A failed test using this data likely means a failed submission on the holdout data."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_fake_background_data.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_fake_background_data.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "8. PreFer_fake_background_data.csv",
    "text": "8. PreFer_fake_background_data.csv\nThis dataset contains the same variables as PreFer_train_background_data.csv, but it includes fake data with randomly drawn values from each variable in the background data. This dataset is used for testing whether the developed preprocessing scripts and trained models work when the original analyses included PreFer_train_background_data.csv. A failed test using this data likely means a failed submission on the holdout data."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_holdout_data.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_holdout_data.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "9. PreFer_holdout_data.csv",
    "text": "9. PreFer_holdout_data.csv\nThe holdout data, only available to PreFer organisers. This dataset contains the same 31634 variables as PreFer_train_data.csv and for 395 respondents. It is the dataset that will be used for assessing predictive ability.\nThis dataset only contains information for 395 respondents born between 1975 and 2002 and for whom the outcome (new_child) was known."
  },
  {
    "objectID": "posts/posts/2024-03-20-prefer-datasets.html#prefer_holdout_outcome.csv",
    "href": "posts/posts/2024-03-20-prefer-datasets.html#prefer_holdout_outcome.csv",
    "title": "A guide on PreFer LISS datasets",
    "section": "10. PreFer_holdout_outcome.csv",
    "text": "10. PreFer_holdout_outcome.csv\nThe outcome for the holdout data, only available to PreFer organisers. This dataset contains the outcomes for the same 395 respondents as those in as PreFer_train_data.csv."
  },
  {
    "objectID": "posts/posts/2024-03-29-submisson-frequent-errors.html",
    "href": "posts/posts/2024-03-29-submisson-frequent-errors.html",
    "title": "Frequent errors during submission, and how to debug",
    "section": "",
    "text": "When you submit your method, it will automatically run on the fake data first (“PreFer_fake_data.csv”). Only if this check is successful, the method will run on the holdout set. Here we will be adding common reasons why your submission might fail to pass this automatic checks.\nNote: always test your method locally on the fake data before submitting (e.g. apply your method to the fake data to see if the method produces predictions) and debug in case of errors. Here we will mostly focus on errors that you might still encounter during this automated check even if your method was working locally)."
  },
  {
    "objectID": "posts/posts/2024-03-29-submisson-frequent-errors.html#how-to-find-out-if-your-method-passed-automatic-checks",
    "href": "posts/posts/2024-03-29-submisson-frequent-errors.html#how-to-find-out-if-your-method-passed-automatic-checks",
    "title": "Frequent errors during submission, and how to debug",
    "section": "How to find out if your method passed automatic checks",
    "text": "How to find out if your method passed automatic checks\nYou can see it on the “Actions” tab in your own github repository. After you update your repository, you will see on this page whether your method passed automatic checks or not. Make sure to allow Github Actions: go to the “Actions” tab and click “I understand my workflows, go ahead and enable them.”"
  },
  {
    "objectID": "posts/posts/2024-03-29-submisson-frequent-errors.html#how-to-find-the-error-message",
    "href": "posts/posts/2024-03-29-submisson-frequent-errors.html#how-to-find-the-error-message",
    "title": "Frequent errors during submission, and how to debug",
    "section": "How to find the error message",
    "text": "How to find the error message\nClick on the failed “run”. You’ll see a message there.\n\nYou can also click on “test” and view more detailed information about the error and the stage at which it occurred."
  },
  {
    "objectID": "posts/posts/2024-03-29-submisson-frequent-errors.html#common-errors",
    "href": "posts/posts/2024-03-29-submisson-frequent-errors.html#common-errors",
    "title": "Frequent errors during submission, and how to debug",
    "section": "Common errors",
    "text": "Common errors\n\n\n\n\n\n\nERROR: failed to solve: failed to read dockerfile: open Dockerfile: no such file or directory\n\n\n\n\n\nThis error appears if you are using one programming language for submission, but the “settings.json” indicates a different language. Most likely you are using R, but you haven’t changed the settings. The default set-up is Python; if you would like to use R, go to settings.json and change {“dockerfile”: “python.Dockerfile”} into {“dockerfile”: “r.Dockerfile”}.\n\n\n\n\n\n\n\n\n\n\nError in eval(predvars, data, env) : object ‘some_variable_name_here’ not found\n\n\n\n\n\nThis error occurs if you added new variables to the model, but did not change the “clean_df” function in the submission script accordingly.\n\n\n\n\n\n\n\n\n\n\nValueError: The feature names should match those that were passed during fit. Feature names unseen at fit time: ‘some_variable_name_here’\n\n\n\n\n\nThis error occurs if you added new variables to the model and updated the “clean_df” function in submission script accordingly, but the model is not updated.\n\n\n\n\n\n\n\n\n\n\nError in library(some_package_name) : there is no package called ‘some_package_name’\n\n\n\n\n\nThis error occurs if you are using an (R) package which is not listed in “packages.R” file. Add this package to this file.\n\n\n\n\nPhoto by Sigmund on Unsplash\nPhoto by Timothy Dykes on Unsplash"
  },
  {
    "objectID": "posts/posts/2024-03-22-read-data.html",
    "href": "posts/posts/2024-03-22-read-data.html",
    "title": "Reading in data",
    "section": "",
    "text": "Here we describe ways of reading-in the data for both Python and R.\nNote that different packages may lead to partly different datasets (for example, with columns of different types), because of the way different packages treat things like missing values, empty strings, and dates."
  },
  {
    "objectID": "posts/posts/2024-03-22-read-data.html#r",
    "href": "posts/posts/2024-03-22-read-data.html#r",
    "title": "Reading in data",
    "section": "R",
    "text": "R\nThere are several ways in which one could read data into R, but some of them are more successful and quicker than others (TL;DR: use data.table::fread).\n\nread.csv\nread.csv works, requires no additional packages, but is very slow.\n\ndata &lt;- read.csv(\"path/to/folder/PreFer_train_data.csv\", row.names = FALSE) # this works but is very slow\n\n\n\nread_csv\nreadr::read_csv from the package readr in principle works, but gets many of the column types wrong with default settings (because, by default, it only bases column types on the first 1000 values present in the variable). Do not run this code data &lt;- readr::read_csv(\"PreFer_train_data.csv\"), but use the following code which explicitly tells read_csv that it must make use of the entire column (i.e., all cases) to make a guess of the column type:\n\nlibrary(readr) # requires install.packages(\"readr\") first\ndata &lt;- readr::read_csv(\"path/to/folder/PreFer_train_data.csv\", guess_max = 6418) # this works but is slow\n# 6418 is the number of rows in the data\n\n\n\nfread\ndata.table::fread from the package data.table works like a charm and is very fast. Some additional arguments are useful to avoid default behaviour.\n\nlibrary(data.table) # requires install.packages(\"data.table\") first\ndata &lt;- data.table::fread(\"path/to/folder/PreFer_train_data.csv\", \n                          keepLeadingZeros = TRUE, # if FALSE adds zeroes to some dates\n                          data.table = FALSE) # returns a data.frame object rather than data.table"
  },
  {
    "objectID": "posts/posts/2024-03-22-read-data.html#python",
    "href": "posts/posts/2024-03-22-read-data.html#python",
    "title": "Reading in data",
    "section": "Python",
    "text": "Python\n\nread_csv from pandas\nread_csv from pandas works, but is slow. Specifying low_memory=False is needed. If low_memory=False, then whole columns are read in first, and then the proper data types in the columns are determined. If low_memory=True (default), then pandas reads in the data in chunks of rows, then appends them together. This results in lower memory use while parsing, but incorrect (mixed) type of data in a column, when for example there are many missing values in a column (which are floating point numbers in python) but all other values are integers.\n```{python}\nimport pandas as pd # requires installing pandas first\ntrain = pd.read_csv(\"path to the data which is NOT in your local repository\\\\PreFer_train_data.csv\", low_memory = False) # this works but is slow\n```\n\n\nread_csv from polars\nread_csv from the polars package takes less time. Polars is a pandas alternative designed to process data faster. If you want to work with a pandas dataframe, use to_pandas() to convert. For that, pyarrow package also needs to be installed.\ninfer_schema_length=6418 is needed to increase the number of lines used for determining column types; 6418 is the number of rows in the data.\n```{python}\nimport polars as pl     # requires installing polars first\nimport pyarrow          # requires installing pyarrow first\ntrain = pd.read_csv(\"path to the data which is NOT in your local repository\\\\PreFer_train_data.csv\", infer_schema_length=6418).to_pandas() \n```"
  },
  {
    "objectID": "posts/posts/2023-12-15-odissei-talk.html",
    "href": "posts/posts/2023-12-15-odissei-talk.html",
    "title": "Introducing the PreFer data challenge [video]",
    "section": "",
    "text": "In this ODISSEI lecture, Gert Stulp talks about PreFer: the reasons for the challenge, its potential for research into fertility behaviour and design of the challenge.\nYou can watch the recording here."
  },
  {
    "objectID": "posts/posts/2024-03-23-using-different-dataset.html",
    "href": "posts/posts/2024-03-23-using-different-dataset.html",
    "title": "A guide on using different datasets",
    "section": "",
    "text": "Here we describe how you can (maybe) fruitfully combine the different datasets PreFer_train_data.csv, PreFer_train_background_data.csv, and PreFer_train_supplementary_data.csv. (here you can find a guide on these different datasets)\nBelow it is assumed that you have these datasets in your working directory."
  },
  {
    "objectID": "posts/posts/2024-03-23-using-different-dataset.html#prefer_train_data.csv",
    "href": "posts/posts/2024-03-23-using-different-dataset.html#prefer_train_data.csv",
    "title": "A guide on using different datasets",
    "section": "PreFer_train_data.csv",
    "text": "PreFer_train_data.csv\nThe most important dataset of the PreFer data challenge is the PreFer_train_data.csv dataset. This contains data on all LISS respondents who were between the ages of 18 and 45 in 2020 (i.e., had birthyears between 1975 and 2002). Click here for an explanation on how to quickly read-in the data\n\nlibrary(data.table) # requires install.packages(\"data.table\") first\ndata &lt;- data.table::fread(\"path/to/folder/PreFer_train_data.csv\", \n                          keepLeadingZeros = TRUE, # if FALSE adds zeroes to some dates\n                          data.table = FALSE) # returns a data.frame object rather than data.table"
  },
  {
    "objectID": "posts/posts/2024-03-23-using-different-dataset.html#prefer_train_background_data.csv",
    "href": "posts/posts/2024-03-23-using-different-dataset.html#prefer_train_background_data.csv",
    "title": "A guide on using different datasets",
    "section": "PreFer_train_background_data.csv",
    "text": "PreFer_train_background_data.csv\nThe PreFer_background_data.csv contains data on particularly variables on a monthly basis for each respondent.\n\nbackground &lt;- data.table::fread(\"path/to/folder/PreFer_train_background_data.csv\", \n                                keepLeadingZeros = TRUE, \n                                data.table = FALSE)"
  },
  {
    "objectID": "posts/posts/2024-03-23-using-different-dataset.html#prefer_train_supplementary_data.csv",
    "href": "posts/posts/2024-03-23-using-different-dataset.html#prefer_train_supplementary_data.csv",
    "title": "A guide on using different datasets",
    "section": "PreFer_train_supplementary_data.csv",
    "text": "PreFer_train_supplementary_data.csv\nThe PreFer_train_supplementary_data.csv contains data on LISS respondents that did not fall in our target group, i.e., had birthyears lower than 1975 or higher than 2002.\n\nsupplement &lt;- data.table::fread(\"path/to/folder/PreFer_train_supplementary_data.csv\", \n                                keepLeadingZeros = TRUE, \n                                data.table = FALSE)"
  },
  {
    "objectID": "posts/posts/2024-03-23-using-different-dataset.html#using-information-from-background-data",
    "href": "posts/posts/2024-03-23-using-different-dataset.html#using-information-from-background-data",
    "title": "A guide on using different datasets",
    "section": "Using information from background data",
    "text": "Using information from background data\nOne could use the background data to enhance the train data (PreFer_train_data.csv). For example, imagine that you would want to add the average income of respondents in the last three months prior to 2021. Here is what one could add to the submission.R script:\n\nlibrary(dplyr) # for data wrangling\n\n# creating average income for all respondents\nincome_3months &lt;- background |&gt;\n  # only select last 3 months\n  filter(wave &gt;= 202010 & wave &lt;= 202012) |&gt;\n  # for each respondent\n  group_by(nomem_encr) |&gt;\n  # calculate average income\n  summarise(mean_income = mean(netinc, na.rm = TRUE))\n\n# add income to train data\ndata &lt;- left_join(data, income_3months, by = \"nomem_encr\")\n\ndata &lt;- data |&gt;\n  # impute mean income if missing\n  mutate(mean_income_imp = if_else(is.na(mean_income), \n                                   mean(mean_income, na.rm = TRUE),\n                                   mean_income))\n\nYou can now make use of the variable mean_income_imp in your train_save_model-file. To make this work don’t forget to add \"dplyr\" to packages.R! Here you can see it in action and that it passes the check."
  },
  {
    "objectID": "posts/posts/2024-03-23-using-different-dataset.html#using-information-from-the-supplementary-data",
    "href": "posts/posts/2024-03-23-using-different-dataset.html#using-information-from-the-supplementary-data",
    "title": "A guide on using different datasets",
    "section": "Using information from the supplementary data",
    "text": "Using information from the supplementary data\nUsing information from the supplementary data is slightly more complex. Remember that the supplementary data (PreFer_train_supplementary_data.csv) consists of all people who are not in the target group (i.e., it consists of respondents with birthyears prior to 1975 and post 2002). In contrast to the background data, where a PreFer_train_background_data.csv is available (to participants) and where aPreFer_holdout_background_data.csv exists (but is not available to PreFer participants), there is no equivalent of the supplementary data for the holdout data. This means that we cannot use any code to process the supplementary data in the function clean_df, because everything inserted into clean_df will be applied to holdout data. Thus, everything you do with the supplementary data must be specified in the training.R file.\nImagine that you also want to include people who were born in 1974 in your train data. You could insert something like the below in your train_save_model function:\n\nlibrary(dplyr) # for data wrangling\n\n# selecting only respondents from 1974\nrespondents_1974 &lt;- supplement |&gt;\n  filter(birthyear_bg == 1974) \n\n# add respondents from 1974\ndata &lt;- bind_rows(data, respondents_1974)\n\nAnd then use your usual method to train on the data. This will work because your method will make use of these new cases, but the structure of the dataset has not changed (no new variables were added). If you would create a novel variable on the basis of the supplementary data and use this variable in your model, it would not work on the holdout data, because that variable will not exist in the holdout data. Here you can see it in action and that it passes the check."
  },
  {
    "objectID": "5platform.html",
    "href": "5platform.html",
    "title": "Submission",
    "section": "",
    "text": "In Phase 1, participants are asked to submit their methods (code used for data preprocessing and training), rather than the predicted values themselves. The submission platform supports Python and R. Methods will be published open source in a GitHub repository. In GitHub methods are automatically run on example data to check for errors. If the checks are successful then the method can be submitted and will be evaluated on the holdout dataset. This workflow fosters computational reproducibility, which was a concern in the Fragile Families Challenge in which participants only submitted their predictions (Liu & Salganik, 2019). This also allows us to run the method on different (or future) variants of the data. For the submissions, participants will use the open-source web platform Next. It allows for reproducible submissions in data challenges in which data is not publicly available, and therefore common solutions like Kaggle are not possible. Instructions on how to submit to the platform and example code will be provided on a website dedicated to the data challenge.\nIn Phase 2, participants cannot make use of the submission platform because the register data is only available within the RA environment, which is why participants are asked to submit predicted values (generated by their method) by saving them in a specified folder inside the RA environment along with the trained model and all scripts used for data preprocessing and model training.\nPhoto by Kelli McClintock on Unsplash"
  },
  {
    "objectID": "details/overview/4submission_evaluation_winners.html",
    "href": "details/overview/4submission_evaluation_winners.html",
    "title": "Submission, evaluation, and winners",
    "section": "",
    "text": "In Phase 1 and track 3 of Phase 2, participants will submit their methods (code used for data preprocessing and training, and the model) rather than the predicted values themselves, along with the description of the method (e.g. approach to selecting the variables and machine learning model and preprocessing the data). If participants performed analyses to interpret their model, e.g. assessed the importance of different predictors for different groups, these scripts should be provided as well.\nFor the submissions, participants will use the open-source web platform Next (the link will be provided to registered participants). It allows for reproducible submissions in data challenges in which data is not publicly available. See instructions on how to submit to the platform and example code in Guides. The submission platform supports Python and R.\nParticipants will be able to make several intermediate submissions (to test how their model is working on the holdout set) before the final submissions. Results of the intermediate submissions will be presented on the leaderboard will be presented at the end of April, mid-May and beginning of June.\nIn tracks 1-2 of Phase 2, participants cannot make use of the submission platform because the register data is only available within the CBS secure remote access (RA) environment. The participants will submit predicted values generated by their method by saving them in a specified folder inside the RA environment along with the trained model, all scripts used for data preprocessing and model training, and a description of the method."
  },
  {
    "objectID": "details/overview/4submission_evaluation_winners.html#submission",
    "href": "details/overview/4submission_evaluation_winners.html#submission",
    "title": "Submission, evaluation, and winners",
    "section": "",
    "text": "In Phase 1 and track 3 of Phase 2, participants will submit their methods (code used for data preprocessing and training, and the model) rather than the predicted values themselves, along with the description of the method (e.g. approach to selecting the variables and machine learning model and preprocessing the data). If participants performed analyses to interpret their model, e.g. assessed the importance of different predictors for different groups, these scripts should be provided as well.\nFor the submissions, participants will use the open-source web platform Next (the link will be provided to registered participants). It allows for reproducible submissions in data challenges in which data is not publicly available. See instructions on how to submit to the platform and example code in Guides. The submission platform supports Python and R.\nParticipants will be able to make several intermediate submissions (to test how their model is working on the holdout set) before the final submissions. Results of the intermediate submissions will be presented on the leaderboard will be presented at the end of April, mid-May and beginning of June.\nIn tracks 1-2 of Phase 2, participants cannot make use of the submission platform because the register data is only available within the CBS secure remote access (RA) environment. The participants will submit predicted values generated by their method by saving them in a specified folder inside the RA environment along with the trained model, all scripts used for data preprocessing and model training, and a description of the method."
  },
  {
    "objectID": "details/overview/4submission_evaluation_winners.html#evaluation",
    "href": "details/overview/4submission_evaluation_winners.html#evaluation",
    "title": "Submission, evaluation, and winners",
    "section": "Evaluation",
    "text": "Evaluation\nTruth criteria\nTo calculate the outcome for the LISS data, we used the data from the “Family and household” Core study about the number of children in 2020, 2021, 2022, and 2023 (“How many living children do you have in total?”). On the basis of these variables, we calculated whether a person had at least one new child in 2021-2023. LISS data from the “Family and Household” 2023 study will be made available only after the end of the data challenge.\nIn the case of CBS, we used the CBS dataset Kindoudertab that links children with their parents and the data containing the year of birth for each resident of the Netherlands. Based on that, for each person in the sample (Dutch population aged 18-45 in 2020), we calculated the number of children born in 2021-2023 and then derived whether or not a person had at least one new child in 2021-2023.\nMetrics\nThe metrics below are used in both phases of the challenge to assess the quality of the predictions (i.e., the difference between the predicted values and the ground truth). These are common metrics for classification tasks (i.e., predictions with binary outcomes).\nAccuracy: the ratio of correct predictions to the total number of predictions made.\nPrecision: The proportion of positive predictions that were actually correct (i.e. the proportion of people who actually had a new child in 2021-2023 of all the people who were predicted to have a new child in this period).\nRecall: The proportion of positive cases that were correctly identified (i.e. the proportion of people who actually had a new child and were predicted to have a new child of all people in the sample who had a new child in 2021-2023).\nF1 score (for the positive class, or having a new child): The harmonic mean of the precision (P) and recall (R).\nFor both phases of the data challenge, all four metrics will be used for the leaderboards (or ranked lists of the predictive performance of the submitted methods on the holdout data). The F1 score leaderboard is the main leaderboard that will be used as the quantitative criteria to determine the winners of the challenge."
  },
  {
    "objectID": "details/overview/4submission_evaluation_winners.html#selection-into-tracks-1-2-of-phase-2-of-the-data-challenge",
    "href": "details/overview/4submission_evaluation_winners.html#selection-into-tracks-1-2-of-phase-2-of-the-data-challenge",
    "title": "Submission, evaluation, and winners",
    "section": "Selection into tracks 1-2 of Phase 2 of the data challenge",
    "text": "Selection into tracks 1-2 of Phase 2 of the data challenge\nF1 score will be used as the main selection criterion for tracks 1-2 of Phase 2 to select approximately 10-20 teams. The score of the best method submitted by a team (the final or one of the intermediate submissions) during Phase 1 will be taken into account.\nTo ensure the representation of different methods in tracks 1-2, an evaluation committee will assess the submissions with the top F1 scores to select those teams that can proceed to these tracks of Phase 2. The evaluation committee will consist of the organisers of the challenge, an expert in fertility research, and a data scientist. Additional conditions:\n\nat least one team member should be able to be physically present for at least a part of Phase 2 in a country where it is allowed to access the CBS RA. These countries include the European Union, Liechtenstein, Norway, and Iceland, and the countries with an adequacy decision (see the list of such countries here.\n\nthis person passes security checks and is approved by CBS."
  },
  {
    "objectID": "details/overview/4submission_evaluation_winners.html#determining-the-winners",
    "href": "details/overview/4submission_evaluation_winners.html#determining-the-winners",
    "title": "Submission, evaluation, and winners",
    "section": "Determining the winners",
    "text": "Determining the winners\nThere will be three winners determined based on the F1 score:\n- one winner for the first phase and track 3 of the second phase\n- one winner for track 1, Phase 2\n- one winner for track 2, Phase 2\nTo recognise other important contributions in furthering the understanding of fertility behaviour, an evaluation committee will also assess the submissions on the basis of qualitative criteria:\n\ninnovativeness: a novel approach using ideas from either social sciences or data science (e.g. using approaches such as transfer learning, still uncommon in the social sciences)\n\nwhether the method improves our understanding of fertility (based on descriptions unpacking the method to gain a deeper understanding). Participants can for example do error analysis, or examining misclassified cases and trying to understand why the method failed to classify them; analyse predictive performance for particular groups; analyse interactions and importance of factors overall and for different groups; identify good predictors that were not considered so far.\n\nTwo additional winners will be selected based on these qualitative criteria.\nAll winners (five teams) will have an opportunity to present their method and results in a plenary session at the ODISSEI Conference for Social Science in the Netherlands in the end of 2024. One representative per team will have the costs of attending the conference covered.\nIt is important to note that while we will select winners to recognise particular contributions and to encourage the development of the best possible methods during the data challenge, the goals of the data challenge can only be achieved through community efforts of all the participants of the challenge. Because of that all the submissions are highly valued and will be recognised in scientific publications based on the challenge.\nPhoto by Kelli McClintock on Unsplash"
  },
  {
    "objectID": "details/overview/1research_questions.html",
    "href": "details/overview/1research_questions.html",
    "title": "The task, goal, and research questions",
    "section": "",
    "text": "The task of PreFer is to predict for people aged 18-45 in 2020, who will have a(nother) child within the following three years (2021-2023) based on the data up to and including 2020.\nThe goal of the data challenge is to assess the current predictability of individual-level fertility and improve our understanding of fertility behaviour.\nThe results of PreFer will be used to answer the following research questions:\n\nHow well can we predict who will have a(nother) child in the short-term future in the Netherlands?\nWhat are the most important predictors of this fertility outcome?\nAre there novel predictors for this fertility outcome, unaccounted for in the existing theoretical literature? (this can include non-linear effects and interactions between predictors)\n\nHow do theory-driven methods compare to data-driven methods in terms of predictive accuracy?\n\nWhat poses larger constraints on predictive ability: the number of cases or the number of (‘subjective’) variables?\n\nSurvey data typically consists of hundreds or thousands of variables (including subjective measures like intentions or values) on a relatively small sample (at least in comparison to data science projects). Population registries typically contain fewer variables only on a set of ‘objective’ measures (e.g., income, education, cohabitation) but describe a large number of people.\n\nTo what extent can predictions on survey data be improved by augmenting it by register data? (e.g. imputing missing values, correcting measurement errors, adding new variables)\n\nTo what extent can predictions based on the register data be improved by augmenting it with survey data (e.g. “subjective” variables)?\n\nFurther details:\n\nGert Stulp talks in more detail about potential of the data challenge for research into fertility behaviour here\nWe describe the advantages of using a prediction-focused approach in the preprint about PreFer.\n\nPhoto by Adam Mosley on Unsplash"
  },
  {
    "objectID": "details/overview/3phases.html",
    "href": "details/overview/3phases.html",
    "title": "Phases of the challenge",
    "section": "",
    "text": "In Phase 1, participants will predict the outcome using only the LISS data. Participants will be able to download the LISS training data on their own devices (after accepting the user statement; all the instructions will be sent to the registered participants) and run their methods on their own computers. They will submit their methods through a submission platform (see Submission).\nPhase 1 will run from April 1 till the end of May, 2024."
  },
  {
    "objectID": "details/overview/3phases.html#phase-1",
    "href": "details/overview/3phases.html#phase-1",
    "title": "Phases of the challenge",
    "section": "",
    "text": "In Phase 1, participants will predict the outcome using only the LISS data. Participants will be able to download the LISS training data on their own devices (after accepting the user statement; all the instructions will be sent to the registered participants) and run their methods on their own computers. They will submit their methods through a submission platform (see Submission).\nPhase 1 will run from April 1 till the end of May, 2024."
  },
  {
    "objectID": "details/overview/3phases.html#phase-2",
    "href": "details/overview/3phases.html#phase-2",
    "title": "Phases of the challenge",
    "section": "Phase 2",
    "text": "Phase 2\nIn mid-June 2024, Phase 2 will start. It will run until mid-September 2024.\nPhase 2 includes three tracks. Based on all the submissions that teams make in the first phase, we will select several of the best-performing teams (10-20 teams) for tracks 1 and 2 of the second phase to work inside the secure Remote Access (RA) environment of CBS, or Statistics Netherlands. You can read about how teams will be selected to participate in tracks 1-2 and the conditions for accessing the CBS data here.\nTeams that are not selected into tracks 1 and 2 will continue working on the LISS data (this is track 3).\nTracks 1 and 2 differ on the holdout set for which the participants will predict the outcome. Participants themselves can choose which track(s) they will work on.\n\nIn the first track, participants will predict the fertility outcome for the LISS holdout set. This is similar to Phase 1/track 3, but the difference is that the LISS data can be linked to CBS data inside the RA environment.\nIn the second track, participants will instead predict the fertility outcome for the CBS holdout set.\n\nThis setup provides the participants of tracks 1 and 2 with a unique opportunity to develop and test multiple approaches to possibly enhance the performance of their methods by using both datasets.\nAccess to the CBS RA environment and CBS data is governed by strict rules and regulations in relation to data protection and privacy. One consequence of such rules is that access to this RA environment is only possible from the European Economic Area and a few other countries and is subject to the approval of CBS and passing security checks. Another issue in working in the CBS RA environment is that computing resources are constrained. Given the limitations, only a selection of teams can participate in the second phase. The costs of access to the CBS datasets will be covered by ODISSEI and access will be subject to the vetting and agreement of Statistics Netherlands and the ODISSEI Management Board under the general grant conditions of ODISSEI."
  },
  {
    "objectID": "about-us.html",
    "href": "about-us.html",
    "title": "The organizers",
    "section": "",
    "text": "PreFer is organised by the University of Groningen, ODISSEI, Eyra, and Centerdata."
  },
  {
    "objectID": "about-us.html#organisers",
    "href": "about-us.html#organisers",
    "title": "The organizers",
    "section": "Organisers",
    "text": "Organisers\nThe data challenge is organised by Gert Stulp, Tom Emery, Javier Garcia Bernardo, Adriënne Mendrik, Joris Mulder, Malvina Nissim, Paulina Pankowska, Elizaveta Sivak."
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "CV",
    "section": "",
    "text": "You can download a pdf-version my CV"
  },
  {
    "objectID": "CV/index.html#education",
    "href": "CV/index.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education"
  },
  {
    "objectID": "CV/index.html#test",
    "href": "CV/index.html#test",
    "title": "CV",
    "section": "TEST",
    "text": "TEST"
  },
  {
    "objectID": "CV/index.html#education-1",
    "href": "CV/index.html#education-1",
    "title": "CV",
    "section": "Education",
    "text": "Education"
  },
  {
    "objectID": "CV/index.html#professional-career",
    "href": "CV/index.html#professional-career",
    "title": "CV",
    "section": "Professional career",
    "text": "Professional career"
  },
  {
    "objectID": "CV/index.html#grants-awards-nominations",
    "href": "CV/index.html#grants-awards-nominations",
    "title": "CV",
    "section": "Grants, awards, nominations",
    "text": "Grants, awards, nominations"
  },
  {
    "objectID": "posts/posts/2024-04-01-testing-predictive-ability-R.html",
    "href": "posts/posts/2024-04-01-testing-predictive-ability-R.html",
    "title": "Calculating predictive accuracy scores in R",
    "section": "",
    "text": "The predictive accuracy scores (F1, accuracy, precision, recall) for all submissions will be calculated via the script score.py. If you work in Python, you can also calculate these scores by running python score.py predictions.csv outcome.csv in the command line, where “predictions.csv” refers to a csv-file that includes two columns (nomem_encr and prediction; the output of predict_outcomes function) and outcome.csv (for example “PreFer_train_outcome.csv”) refers to a csv-file that contains the ground truth (also with two columns; nomem_encr and new_child).\nIf you work in R, you cannot do this (unless you save your predictions from submission.R in a csv file and then run the Python code). That’s why we provide a script to calculate these scores yourself. The reason that we do not include this R-script in the repository, is because when you submit your model/method, the Python-script will be used to calculate the outcomes for your submission during the challenge. Although entirely unlikely, for whatever reason the Python- and R-script that produce the scores may lead to different results. So use this R-script for convenience, but note the disclaimer above."
  },
  {
    "objectID": "posts/posts/2024-04-01-testing-predictive-ability-R.html#manually",
    "href": "posts/posts/2024-04-01-testing-predictive-ability-R.html#manually",
    "title": "Calculating predictive accuracy scores in R",
    "section": "Manually",
    "text": "Manually\n\nAccuracy: fraction of correct predictions out of total predictions made. Number of people who did not have a child who were correctly predicted not to have births (6 in our example) plus the number of people who had a child and were correctly predicted to have a child (7 in our example), divided by the total number of people (20). Thus, the accuracy for our example is \\(\\frac{6+7}{20}=0.65\\)\nPrecision: fraction of correct predictions out of total number of predicted cases “of interest”. Among those who were predicted to have child (11 in our case), the percentage who indeed had a child (7 out of those 11 had a child), thus 0.64. Sometimes this is phrased as the \\(\\frac{\\text{true positives}}{\\text{true positives + false positives}} =\\frac{7}{7+4}\\).\nRecall: fraction of correct predictions out of total number of cases “of interest”. Among those who had a child (10 in our case), the percentage who were predicted to have a child (7 out of those 10 had a child), thus 0.7. Sometimes this is phrased as the \\(\\frac{\\text{true positives}}{\\text{true positives + false false negatives}} =\\frac{7}{7+3}\\).\nF1-score: the harmonic mean between precision and recall: \\(2\\frac{precision*recall}{precision+recall}\\). Here \\(2*\\frac{0.64*0.7}{0.64*0.7}=0.67\\)."
  },
  {
    "objectID": "posts/posts/2024-04-01-testing-predictive-ability-R.html#via-the-script",
    "href": "posts/posts/2024-04-01-testing-predictive-ability-R.html#via-the-script",
    "title": "Calculating predictive accuracy scores in R",
    "section": "Via the script",
    "text": "Via the script\n\nsource(\"https://preferdatachallenge.nl/data/score.R\")\n\n\nscore(predictions_df, truth_df)\n\n  accuracy precision recall  f1_score\n1     0.65 0.6363636    0.7 0.6666667"
  },
  {
    "objectID": "posts/posts/2024-03-30-slightly-complexer-walkthrough-r.html",
    "href": "posts/posts/2024-03-30-slightly-complexer-walkthrough-r.html",
    "title": "A walkthough of submitting a simple model in R",
    "section": "",
    "text": "Here we describe how to prepare and make a slighty more complex submission in R than this one. The sole purpose of this script is to make the submission process more clear by showing some of the errors that you might bump into, not to present a model that is any good.\nOur aim is to run a penalized regression via the package glmnet. We’re only doing a penalized regression with three variables, birthyear_bg, gender_bg, and oplmet_2020."
  },
  {
    "objectID": "posts/posts/2024-03-30-slightly-complexer-walkthrough-r.html#testing-clean_df",
    "href": "posts/posts/2024-03-30-slightly-complexer-walkthrough-r.html#testing-clean_df",
    "title": "A walkthough of submitting a simple model in R",
    "section": "Testing clean_df",
    "text": "Testing clean_df\nLet’s test if our clean_df function works. Let’s read in the PreFer_train_data.csv (which you hopefully have in a different folder than your local repository; see step 1 here {target=“_blank”}). Let’s also read-in the outcome for the training data and the fake training data, which we’ll both use.\n\nlibrary(data.table)\ntrain &lt;- data.table::fread(\"../../../PreFer_data/PreFer_train_data.csv\", \n                           keepLeadingZeros = TRUE, # if FALSE adds zeroes to some dates\n                           data.table = FALSE) # returns a data.frame object rather than data.table \noutcome &lt;- data.table::fread(\"../../../PreFer_data/PreFer_train_outcome.csv\", \n                           keepLeadingZeros = TRUE, data.table = FALSE)\nfake &lt;- data.table::fread(\"../../../PreFer_data/PreFer_fake_data.csv\", \n                          keepLeadingZeros = TRUE, data.table = FALSE)\n\nclean &lt;- clean_df(train)\nstr(clean)\n\n num [1:2939, 1:12] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:2939] \"2\" \"3\" \"5\" \"9\" ...\n  ..$ : chr [1:12] \"(Intercept)\" \"nomem_encr\" \"birthyear_bg\" \"gender_bg2\" ...\n - attr(*, \"assign\")= int [1:12] 0 1 2 3 4 4 4 4 4 4 ...\n - attr(*, \"contrasts\")=List of 2\n  ..$ gender_bg  : chr \"contr.treatment\"\n  ..$ oplmet_2020: chr \"contr.treatment\"\n\n\nA matrix with 2939 cases and 12 variables."
  }
]