{
  "hash": "c25d19a8ca279572ba76b3b54ce296b2",
  "result": {
    "markdown": "---\ntitle: \"Calculating predictive accuracy scores in R\"\ndescription: |\n  Here we show to evaluate your model in terms of predictive accuracy when you are working in R.\ncategories:\n  - prediction\n  - guide\nauthor: \n  - \"Gert Stulp\"\n  - \"Lisa Sivak\"\ndate: '2024-04-01'\ntoc: true\nimage: ../../images/duck2.png\nimage-alt: LISS logo. \nlanguage: \n    section-title-footnotes: References\n---\n\n\nThe predictive accuracy scores (F1, accuracy, precision, recall) for all submissions will be calculated via the script `score.py`. If you work in Python, you can also calculate these scores by running `python score.py predictions.csv outcome.csv` in the command line, where \"predictions.csv\" refers to a csv-file that includes two columns (nomem_encr and prediction; the output of `predict_outcomes` function) and `outcome.csv` (for example \"PreFer_train_outcome.csv\") refers to a csv-file that contains the ground truth (also with two columns; nomem_encr and new_child). \n\nIf you work in R, you cannot do this (unless you save your predictions from submission.R in a csv file and then run the Python code). That's why we provide a script to calculate these scores yourself. The reason that we do not include this R-script in the repository, is because when you submit your model/method, the Python-script will be used to calculate the outcomes for your submission during the challenge. Although entirely unlikely, for whatever reason the Python- and R-script that produce the scores may lead to different results. So use this R-script for convenience, but note the disclaimer above.\n\n# score.R\nYou can just `source` this function into your R-environment by running: `source(\"https://preferdatachallenge.nl/data/score.R\")`. This R-script contains the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predictions_df = data.frame with two columns; nomem_encr and predictions\n# ground_truth_df = data.frame with two columns; nomem_encr and new_child\n\nscore <- function(predictions_df, ground_truth_df) {\n  \n  merged_df = merge(predictions_df, ground_truth_df, by = \"nomem_encr\",\n                    all.y = TRUE) # right join\n  \n  # Calculate accuracy\n  accuracy = sum( merged_df$prediction == merged_df$new_child) / length(merged_df$new_child) \n  \n  # Calculate true positives, false positives, and false negatives\n  true_positives = sum( merged_df$prediction == 1 & merged_df$new_child == 1 )\n    \n  false_positives = sum( merged_df$prediction == 1 & merged_df$new_child == 0 )\n    \n  false_negatives = sum( merged_df$prediction == 0 & merged_df$new_child == 1 )\n    \n  # Calculate precision, recall, and F1 score\n  if( (true_positives + false_positives) == 0) {\n    precision = 0\n  } else {\n    precision = true_positives / (true_positives + false_positives)\n  }\n  if( (true_positives + false_negatives) == 0) {\n    recall = 0\n  } else {\n    recall = true_positives / (true_positives + false_negatives)\n  }\n  if( (precision + recall) == 0) {\n    f1_score = 0\n  } else {\n    f1_score = 2 * (precision * recall) / (precision + recall)\n  }\n  \n  metrics_df <- data.frame(\n    'accuracy' = accuracy,\n    'precision' = precision,\n    'recall' = recall,\n    'f1_score' = f1_score\n  )\n  \n  return(metrics_df)\n\n}\n```\n:::\n\n\n\n# Creating fake data\nLet's create fake data to use for our script. \n\n::: {.cell}\n\n```{.r .cell-code}\n# the names of these variables for both datasets are set\ntruth_df <- data.frame(\n  nomem_encr = 1:20, # 20 fake IDs\n  new_child  = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) # 10 people had child, 10 people did not\n)\npredictions_df <- data.frame(\n  nomem_encr = 1:20, # same fake IDs\n  prediction = c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n                 0, 0, 0, 1, 1, 1, 1, 1, 1, 1)\n)\n```\n:::\n\n\n# Scores\nThe following scores are created: F1, accuracy, precision, recall, using counts of true positives, false positives, and false negatives.\n\n## Manually\n- **Accuracy**: fraction of correct predictions out of total predictions made. Number of people who did not have a child who were correctly predicted not to have births (6 in our example) plus the number of people who had a child and were correctly predicted to have a child (7 in our example), divided by the total number of people (20). Thus, the accuracy for our example is $\\frac{6+7}{20}=0.65$\n\n- **Precision**: fraction of correct predictions out of total number of predicted cases \"of interest\". Among those who were predicted to have child (11 in our case), the percentage who indeed had a child (7 out of those 11 had a child), thus 0.64. Sometimes this is phrased as the $\\frac{\\text{true positives}}{\\text{true positives + false positives}} =\\frac{7}{7+4}$.\n\n- **Recall**: fraction of correct predictions out of total number of cases \"of interest\". Among those who had a child (10 in our case), the percentage who were predicted to have a child (7 out of those 10 had a child), thus 0.7. Sometimes this is phrased as the $\\frac{\\text{true positives}}{\\text{true positives + false false negatives}} =\\frac{7}{7+3}$.\n\n- **F1-score**: the harmonic mean between precision and recall: $2\\frac{precision*recall}{precision+recall}$. Here $2*\\frac{0.64*0.7}{0.64*0.7}=0.67$.\n\n## Via the script\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"https://preferdatachallenge.nl/data/score.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nscore(predictions_df, truth_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  accuracy precision recall  f1_score\n1     0.65 0.6363636    0.7 0.6666667\n```\n:::\n:::\n",
    "supporting": [
      "2024-04-01-testing-predictive-ability-R_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}