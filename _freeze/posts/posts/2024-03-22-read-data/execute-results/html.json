{
  "hash": "c916b4a9ca10a0c942174b7e9a1eafa4",
  "result": {
    "markdown": "---\ntitle: \"Reading in data\"\ndescription: |\n  Here is some useful information on reading in / load PreFer data.   \ncategories:\n  - dataset\n  - guide\nauthor: \n  - \"Gert Stulp\"\n  - \"Lisa Sivak\"\ndate: '2024-03-22'\ntoc: true\nimage: ../../images/liss_pick.png\nimage-alt: LISS logo. \nlanguage: \n    section-title-footnotes: References\n---\n\n\n\nHere we describe ways of reading-in the data for both Python and R. \n\n# Reading in the data\n\nThe most important dataset of the PreFer data challenge is the `PreFer_train_data.csv` dataset ([click here](2024-03-20-prefer-datasets.qmd) for information on the datasets that are available). This contains data on all LISS respondents who were between the ages of 18 and 45 in 2020 (i.e., had birthyears between 1975 and 2002). We will see how we can read-in this data with the help of R or Python. \n\n\n## R\n\nThere are several ways in which one could read data into R, but some of them are more successful and quicker than others (TL;DR: use `data.table::fread`. The below code assumes that you have the data `PreFer_train_data.csv` in your working directory.   \n\n### read.csv\n`read.csv` works, requires no additional packages, but is *very* slow. \n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read.csv(\"path/to/folder/PreFer_train_data.csv\", row.names = FALSE) # this works but is very slow\n```\n:::\n\n\n### read_csv\n`readr::read_csv` from the package `readr` in principle works, but gets many of the column types wrong with default settings (because, by default, it only bases column types on the first 1000 values present in the variable). Do not run this code `data <- readr::read_csv(\"PreFer_train_data.csv\")`, but use the following code which explicitly tells `read_csv` that it must make use of the entire column (i.e., all cases) to make a guess of the column type:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) # requires install.packages(\"readr\") first\ndata <- readr::read_csv(\"path/to/folder/PreFer_train_data.csv\", guess_max = 6418) # this works but is slow\n# 6418 is the number of rows in the data\n```\n:::\n\n\n### fread\n`data.table::fread` from the package `data.table` works like a charm and is very fast. Some additional arguments are useful to avoid default behaviour. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table) # requires install.packages(\"data.table\") first\ndata <- data.table::fread(\"path/to/folder/PreFer_train_data.csv\", \n                          keepLeadingZeros = TRUE, # if FALSE adds zeroes to some dates\n                          data.table = FALSE) # returns a data.frame object rather than data.table \n```\n:::\n\n\n\n## Python\n\n### read_csv from pandas\n`pd.read_csv` works, but is slow. Specifying `low_memory=False` is needed. If `low_memory=False`, then whole columns are read in first, and then the proper data types in the columns are determined. If `low_memory=True` (default), then pandas reads in the data in chunks of rows, then appends them together. This results in lower memory use while parsing, but incorrect (mixed) type of data in a column, when for example there are many missing values in a column (which are floating point numbers in python) and a few integers.  \n```{{python}}\nimport pandas as pd\ntrain = pd.read_csv(\"path to the data which is NOT in your local repository\\\\PreFer_train_data.csv\", low_memory = False) # this works but is slow\n```\n\n\n### read_csv from polars (or pl) \n`read_csv` from the polars package is faster. Polars DataFrame can be then converted into a pandas DataFrame using `to_pandas()`. For that, `pyarrow` package also needs to be installed.  \n```{{python}}\nimport polars as pl\ntrain = pd.read_csv(\"path to the data which is NOT in your local repository\\\\PreFer_train_data.csv\", infer_schema_length=10000).to_pandas() \n```",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}